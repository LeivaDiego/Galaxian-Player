{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9c7aaa2",
   "metadata": {},
   "source": [
    "# Entrenamiento Completo para Dueling DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6859b98",
   "metadata": {},
   "source": [
    "## Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4833495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from gym_envs import make_galaxian_env\n",
    "from utils import set_global_seed, get_seed\n",
    "from models import DuelingDQN\n",
    "from replay_buffer import ReplayBuffer\n",
    "from policies import DQNPolicy\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839f8a2b",
   "metadata": {},
   "source": [
    "## Configuración de CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2caed05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando SEED global = 42\n",
      "Using GPU: NVIDIA GeForce RTX 3070 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "seed = get_seed()\n",
    "if seed is not None:\n",
    "    print(f\"Usando SEED global = {seed}\")\n",
    "    set_global_seed(seed)\n",
    "else:\n",
    "    # Crear una semilla aleatoria y usarla\n",
    "    seed = np.random.randint(0, 10000)\n",
    "    set_global_seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d61df54",
   "metadata": {},
   "source": [
    "## Helper para convertir obs → estado (C, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d48c4faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obs_to_state(obs) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convierte la observación del entorno (que puede venir como LazyFrames\n",
    "    u otro tipo) a un np.ndarray con shape (C, H, W).\n",
    "    Args:\n",
    "        obs: observación del entorno\n",
    "    Returns:\n",
    "        np.ndarray con shape (C, H, W)\n",
    "    \"\"\"\n",
    "    arr = np.array(obs, copy=False)\n",
    "\n",
    "    if arr.ndim == 3:\n",
    "        # Casos típicos:\n",
    "        # - (H, W, C)\n",
    "        # - (C, H, W)\n",
    "        H, W = 84, 84  # por diseño de AtariPreprocessing\n",
    "\n",
    "        if arr.shape[0] == H and arr.shape[1] == W:\n",
    "            # (H, W, C) -> (C, H, W)\n",
    "            arr = np.transpose(arr, (2, 0, 1))\n",
    "        elif arr.shape[1] == H and arr.shape[2] == W:\n",
    "            # ya está en (C, H, W), no tocamos\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(f\"Shape de obs inesperado: {arr.shape}\")\n",
    "    elif arr.ndim == 2:\n",
    "        # 1 frame (H, W) -> (1, H, W)\n",
    "        arr = arr[None, ...]\n",
    "    else:\n",
    "        raise ValueError(f\"Obs con ndim no soportado: {arr.ndim}\")\n",
    "\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e10b427",
   "metadata": {},
   "source": [
    "## Función de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60a9197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn_variant(\n",
    "    model_class,                 # DQN o DuelingDQN\n",
    "    num_episodes: int,\n",
    "    max_steps: int,\n",
    "    label: str = \"DQN\",\n",
    "    seed_base: int | None = None,\n",
    "    print_every: int = 10,\n",
    "    checkpoint_every: int = 100,\n",
    "):\n",
    "    \"\"\"\n",
    "    Entrena un modelo DQN-like (DQN o DuelingDQN) en Galaxian.\n",
    "    Crea:\n",
    "        - nuevo env\n",
    "        - nuevo modelo online/target\n",
    "        - nuevo replay buffer\n",
    "        - su propio loop de entrenamiento\n",
    "\n",
    "    Args:\n",
    "        - model_class: clase del modelo a usar (DQN o DuelingDQN)\n",
    "        - num_episodes: número de episodios para entrenar\n",
    "        - max_steps: número máximo de pasos por episodio\n",
    "        - label: etiqueta para identificar la variante en los logs\n",
    "        - seed_base: semilla base para reproducibilidad (opcional)\n",
    "\n",
    "    Returns:\n",
    "        - lista de recompensas por episodio\n",
    "        - lista de longitudes de episodio\n",
    "    \"\"\"\n",
    "    # Crear entorno DeepMind preprocesado\n",
    "    env = make_galaxian_env(\n",
    "        render_mode=None,           # para entrenamiento, más rápido\n",
    "        deepmind_wrappers=True,     # aplica AtariPreprocessing + FrameStackObservation\n",
    "        frame_stack=4,              # número de frames a apilar\n",
    "    )\n",
    "\n",
    "    # Reset inicial para definir input shape\n",
    "    if seed_base is not None:\n",
    "        obs, _ = env.reset(seed=seed_base)\n",
    "    else:\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "    state = obs_to_state(obs)\n",
    "    in_channels = state.shape[0]\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    # Redes online y target\n",
    "    online_net = model_class(in_channels=in_channels, num_actions=num_actions).to(device)\n",
    "    target_net = model_class(in_channels=in_channels, num_actions=num_actions).to(device)\n",
    "    target_net.load_state_dict(online_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    # Optimizador\n",
    "    optimizer = optim.Adam(online_net.parameters(), lr=1e-4)\n",
    "\n",
    "    # Hiperparámetros\n",
    "    gamma = 0.99\n",
    "    batch_size = 32\n",
    "\n",
    "    # Buffer de replay grande para estabilidad\n",
    "    buffer_capacity = 350_000\n",
    "\n",
    "    # Empezar a aprender cuando ya haya suficiente diversidad en el buffer\n",
    "    start_learning_after = 50_000\n",
    "\n",
    "    # Frecuencia de actualización de la red target\n",
    "    target_update_freq = 10_000\n",
    "\n",
    "    # Decaimiento de epsilon\n",
    "    epsilon_start = 1.0\n",
    "    epsilon_end = 0.1\n",
    "    epsilon_decay_steps = 1_000_000\n",
    "\n",
    "    # Replay buffer\n",
    "    obs_shape = state.shape\n",
    "    replay_buffer = ReplayBuffer(\n",
    "        capacity=buffer_capacity,\n",
    "        obs_shape=obs_shape,\n",
    "        device=device,\n",
    "        dtype=np.uint8,\n",
    "    )\n",
    "\n",
    "    print(f\"[{label}] max_steps={max_steps}\")\n",
    "    print(f\"[{label}] buffer_capacity={buffer_capacity}\")\n",
    "    print(f\"[{label}] start_learning_after={start_learning_after}\")\n",
    "    print(f\"[{label}] target_update_freq={target_update_freq}\")\n",
    "    print(f\"[{label}] epsilon_decay_steps={epsilon_decay_steps}\")\n",
    "\n",
    "    def epsilon_by_step(step: int) -> float:\n",
    "        \"\"\"\n",
    "        Calcula el valor de epsilon para el paso dado.\n",
    "        Args:\n",
    "            step: paso global actual\n",
    "        Returns:\n",
    "            valor de epsilon\n",
    "        \"\"\"\n",
    "        if step >= epsilon_decay_steps:\n",
    "            return epsilon_end\n",
    "        else:\n",
    "            return epsilon_start - (epsilon_start - epsilon_end) * (step / epsilon_decay_steps)\n",
    "\n",
    "    def select_action(state: np.ndarray, step: int) -> int:\n",
    "        \"\"\"\n",
    "        Selecciona una acción usando una política epsilon-greedy.\n",
    "        Args:\n",
    "            state: estado actual\n",
    "            step: paso global actual\n",
    "        Returns:\n",
    "            acción seleccionada\n",
    "        \"\"\"\n",
    "        eps = epsilon_by_step(step)\n",
    "        if np.random.rand() < eps:\n",
    "            return np.random.randint(0, num_actions)\n",
    "        else:\n",
    "            state_t = torch.as_tensor(state, device=device).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                q_values = online_net(state_t)\n",
    "                action = int(q_values.argmax(dim=1).item())\n",
    "            return action\n",
    "\n",
    "    def dqn_update():\n",
    "        \"\"\"\n",
    "        Realiza una actualización del modelo DQN usando un batch de la replay buffer.\n",
    "        Returns:\n",
    "            valor de la pérdida (loss) o None si no se puede actualizar\n",
    "        \"\"\"\n",
    "        if len(replay_buffer) < batch_size:\n",
    "            return None\n",
    "\n",
    "        states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "        q_values = online_net(states)\n",
    "        q_sa = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_q_values = target_net(next_states)\n",
    "            max_next_q = next_q_values.max(dim=1).values\n",
    "            target = rewards + gamma * (1.0 - dones) * max_next_q\n",
    "\n",
    "        loss = nn.SmoothL1Loss()(q_sa, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    # Loop de entrenamiento\n",
    "    global_step = 0\n",
    "    rewards_per_episode = []\n",
    "    episode_lengths = []\n",
    "\n",
    "    print(f\"\\n[TRAIN] Iniciando entrenamiento para {label}\")\n",
    "\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        # si queremos reproducibilidad entre variantes, podemos desplazar la seed\n",
    "        if seed_base is not None:\n",
    "            obs, _ = env.reset(seed=seed_base + episode)\n",
    "        else:\n",
    "            obs, _ = env.reset()\n",
    "\n",
    "        state = obs_to_state(obs)\n",
    "        done = False\n",
    "        episode_reward = 0.0\n",
    "        steps_in_episode = 0\n",
    "\n",
    "        if global_step >= max_steps:\n",
    "            print(f\"[{label}] Alcanzado max_steps={max_steps}. Fin del entrenamiento.\")\n",
    "            break\n",
    "\n",
    "        while not done and global_step < max_steps:\n",
    "            action = select_action(state, global_step)\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            r_env = reward\n",
    "            r_clip = np.clip(r_env, -1.0, 1.0)\n",
    "\n",
    "            next_state = obs_to_state(next_obs)\n",
    "\n",
    "            replay_buffer.add(\n",
    "                state=state,\n",
    "                action=action,\n",
    "                reward=r_clip,\n",
    "                next_state=next_state,\n",
    "                done=done,\n",
    "            )\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += r_env\n",
    "            global_step += 1\n",
    "            steps_in_episode += 1\n",
    "\n",
    "            if global_step > start_learning_after:\n",
    "                _ = dqn_update()\n",
    "\n",
    "            if global_step % target_update_freq == 0:\n",
    "                target_net.load_state_dict(online_net.state_dict())\n",
    "\n",
    "            if global_step >= max_steps:\n",
    "                done = True\n",
    "                break\n",
    "\n",
    "        rewards_per_episode.append(episode_reward)\n",
    "        episode_lengths.append(steps_in_episode)\n",
    "        \n",
    "        if episode % print_every == 0 or episode == 1 or global_step >= max_steps:\n",
    "            print(\n",
    "                f\"[{label}] Ep {episode:4d} | Step {global_step:7d} | \"\n",
    "                f\"Reward: {episode_reward:5.1f} | Eps: {epsilon_by_step(global_step):.3f}\"\n",
    "            )\n",
    "        \n",
    "        if episode % checkpoint_every == 0 or global_step >= max_steps or episode == 1:\n",
    "            ckpt_path = os.path.join(\n",
    "                    '../checkpoints/',\n",
    "                    f\"{label}_ep{episode:04d}_step{global_step}.pt\",\n",
    "                )\n",
    "            torch.save(\n",
    "                    {\n",
    "                        \"model_state_dict\": online_net.state_dict(),\n",
    "                        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                        \"global_step\": global_step,\n",
    "                        \"episode\": episode,\n",
    "                        \"gamma\": gamma,\n",
    "                        \"batch_size\": batch_size,\n",
    "                        \"buffer_capacity\": buffer_capacity,\n",
    "                        \"start_learning_after\": start_learning_after,\n",
    "                        \"target_update_freq\": target_update_freq,\n",
    "                        \"epsilon_start\": epsilon_start,\n",
    "                        \"epsilon_end\": epsilon_end,\n",
    "                        \"epsilon_decay_steps\": epsilon_decay_steps,\n",
    "                    },\n",
    "                    ckpt_path,\n",
    "                )\n",
    "\n",
    "    env.close()\n",
    "    return rewards_per_episode, episode_lengths, online_net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc7fe99",
   "metadata": {},
   "source": [
    "## Loop de Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f503a4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1_000_000            # para pruebas\n",
    "max_steps = 1_500_000               # steps totales por variante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee6df5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DuelingDQN_Final] max_steps=1500000\n",
      "[DuelingDQN_Final] buffer_capacity=1000000\n",
      "[DuelingDQN_Final] start_learning_after=50000\n",
      "[DuelingDQN_Final] target_update_freq=10000\n",
      "[DuelingDQN_Final] epsilon_decay_steps=900000\n",
      "\n",
      "[TRAIN] Iniciando entrenamiento para DuelingDQN_Final\n",
      "[DuelingDQN_Final] Ep    1 | Step     273 | Reward: 280.0 | Eps: 1.000\n",
      "[DuelingDQN_Final] Ep   10 | Step    6226 | Reward: 630.0 | Eps: 0.993\n",
      "[DuelingDQN_Final] Ep   20 | Step   12401 | Reward: 560.0 | Eps: 0.987\n",
      "[DuelingDQN_Final] Ep   30 | Step   18552 | Reward: 870.0 | Eps: 0.980\n",
      "[DuelingDQN_Final] Ep   40 | Step   23532 | Reward: 300.0 | Eps: 0.975\n",
      "[DuelingDQN_Final] Ep   50 | Step   28860 | Reward: 1400.0 | Eps: 0.970\n",
      "[DuelingDQN_Final] Ep   60 | Step   34361 | Reward: 1280.0 | Eps: 0.964\n",
      "[DuelingDQN_Final] Ep   70 | Step   39972 | Reward: 540.0 | Eps: 0.958\n",
      "[DuelingDQN_Final] Ep   80 | Step   44293 | Reward: 720.0 | Eps: 0.953\n",
      "[DuelingDQN_Final] Ep   90 | Step   51255 | Reward: 890.0 | Eps: 0.946\n",
      "[DuelingDQN_Final] Ep  100 | Step   56041 | Reward: 330.0 | Eps: 0.941\n",
      "[DuelingDQN_Final] Ep  110 | Step   61566 | Reward: 1160.0 | Eps: 0.935\n",
      "[DuelingDQN_Final] Ep  120 | Step   66189 | Reward: 780.0 | Eps: 0.930\n",
      "[DuelingDQN_Final] Ep  130 | Step   72231 | Reward: 1010.0 | Eps: 0.924\n",
      "[DuelingDQN_Final] Ep  140 | Step   78901 | Reward: 860.0 | Eps: 0.917\n",
      "[DuelingDQN_Final] Ep  150 | Step   85130 | Reward: 420.0 | Eps: 0.910\n",
      "[DuelingDQN_Final] Ep  160 | Step   90397 | Reward: 1120.0 | Eps: 0.905\n",
      "[DuelingDQN_Final] Ep  170 | Step   94530 | Reward: 820.0 | Eps: 0.900\n",
      "[DuelingDQN_Final] Ep  180 | Step   99805 | Reward: 1250.0 | Eps: 0.895\n",
      "[DuelingDQN_Final] Ep  190 | Step  104831 | Reward: 1040.0 | Eps: 0.889\n",
      "[DuelingDQN_Final] Ep  200 | Step  109921 | Reward: 670.0 | Eps: 0.884\n",
      "[DuelingDQN_Final] Ep  210 | Step  114526 | Reward: 300.0 | Eps: 0.879\n",
      "[DuelingDQN_Final] Ep  220 | Step  119656 | Reward: 530.0 | Eps: 0.874\n",
      "[DuelingDQN_Final] Ep  230 | Step  123939 | Reward: 480.0 | Eps: 0.869\n",
      "[DuelingDQN_Final] Ep  240 | Step  130090 | Reward: 450.0 | Eps: 0.863\n",
      "[DuelingDQN_Final] Ep  250 | Step  134172 | Reward: 830.0 | Eps: 0.858\n",
      "[DuelingDQN_Final] Ep  260 | Step  138995 | Reward: 310.0 | Eps: 0.853\n",
      "[DuelingDQN_Final] Ep  270 | Step  143088 | Reward: 300.0 | Eps: 0.849\n",
      "[DuelingDQN_Final] Ep  280 | Step  148629 | Reward: 940.0 | Eps: 0.843\n",
      "[DuelingDQN_Final] Ep  290 | Step  154127 | Reward: 660.0 | Eps: 0.837\n",
      "[DuelingDQN_Final] Ep  300 | Step  160190 | Reward: 710.0 | Eps: 0.831\n",
      "[DuelingDQN_Final] Ep  310 | Step  165373 | Reward: 450.0 | Eps: 0.825\n",
      "[DuelingDQN_Final] Ep  320 | Step  171179 | Reward: 760.0 | Eps: 0.819\n",
      "[DuelingDQN_Final] Ep  330 | Step  175737 | Reward: 900.0 | Eps: 0.814\n",
      "[DuelingDQN_Final] Ep  340 | Step  181249 | Reward: 660.0 | Eps: 0.809\n",
      "[DuelingDQN_Final] Ep  350 | Step  186524 | Reward: 600.0 | Eps: 0.803\n",
      "[DuelingDQN_Final] Ep  360 | Step  191767 | Reward: 590.0 | Eps: 0.798\n",
      "[DuelingDQN_Final] Ep  370 | Step  197229 | Reward: 990.0 | Eps: 0.792\n",
      "[DuelingDQN_Final] Ep  380 | Step  203131 | Reward: 1530.0 | Eps: 0.786\n",
      "[DuelingDQN_Final] Ep  390 | Step  210616 | Reward: 360.0 | Eps: 0.778\n",
      "[DuelingDQN_Final] Ep  400 | Step  216828 | Reward: 1170.0 | Eps: 0.771\n",
      "[DuelingDQN_Final] Ep  410 | Step  222107 | Reward: 1200.0 | Eps: 0.766\n",
      "[DuelingDQN_Final] Ep  420 | Step  227283 | Reward: 260.0 | Eps: 0.760\n",
      "[DuelingDQN_Final] Ep  430 | Step  235206 | Reward: 660.0 | Eps: 0.752\n",
      "[DuelingDQN_Final] Ep  440 | Step  239720 | Reward: 330.0 | Eps: 0.747\n",
      "[DuelingDQN_Final] Ep  450 | Step  247002 | Reward: 940.0 | Eps: 0.739\n",
      "[DuelingDQN_Final] Ep  460 | Step  252175 | Reward: 540.0 | Eps: 0.734\n",
      "[DuelingDQN_Final] Ep  470 | Step  256893 | Reward: 330.0 | Eps: 0.729\n",
      "[DuelingDQN_Final] Ep  480 | Step  263961 | Reward: 730.0 | Eps: 0.721\n",
      "[DuelingDQN_Final] Ep  490 | Step  269819 | Reward: 1170.0 | Eps: 0.715\n",
      "[DuelingDQN_Final] Ep  500 | Step  275980 | Reward: 340.0 | Eps: 0.709\n",
      "[DuelingDQN_Final] Ep  510 | Step  282390 | Reward: 870.0 | Eps: 0.702\n",
      "[DuelingDQN_Final] Ep  520 | Step  289145 | Reward: 880.0 | Eps: 0.695\n",
      "[DuelingDQN_Final] Ep  530 | Step  295615 | Reward: 1220.0 | Eps: 0.688\n",
      "[DuelingDQN_Final] Ep  540 | Step  300548 | Reward: 770.0 | Eps: 0.683\n",
      "[DuelingDQN_Final] Ep  550 | Step  305446 | Reward: 480.0 | Eps: 0.678\n",
      "[DuelingDQN_Final] Ep  560 | Step  314434 | Reward: 480.0 | Eps: 0.668\n",
      "[DuelingDQN_Final] Ep  570 | Step  320943 | Reward: 540.0 | Eps: 0.661\n",
      "[DuelingDQN_Final] Ep  580 | Step  326882 | Reward: 1160.0 | Eps: 0.655\n",
      "[DuelingDQN_Final] Ep  590 | Step  332624 | Reward: 1240.0 | Eps: 0.649\n",
      "[DuelingDQN_Final] Ep  600 | Step  339030 | Reward: 1470.0 | Eps: 0.642\n",
      "[DuelingDQN_Final] Ep  610 | Step  346746 | Reward: 1140.0 | Eps: 0.634\n",
      "[DuelingDQN_Final] Ep  620 | Step  353980 | Reward: 800.0 | Eps: 0.626\n",
      "[DuelingDQN_Final] Ep  630 | Step  359714 | Reward: 1070.0 | Eps: 0.620\n",
      "[DuelingDQN_Final] Ep  640 | Step  367294 | Reward: 760.0 | Eps: 0.612\n",
      "[DuelingDQN_Final] Ep  650 | Step  374335 | Reward: 1520.0 | Eps: 0.605\n",
      "[DuelingDQN_Final] Ep  660 | Step  381269 | Reward: 740.0 | Eps: 0.598\n",
      "[DuelingDQN_Final] Ep  670 | Step  387556 | Reward: 770.0 | Eps: 0.591\n",
      "[DuelingDQN_Final] Ep  680 | Step  395213 | Reward: 1400.0 | Eps: 0.583\n",
      "[DuelingDQN_Final] Ep  690 | Step  403168 | Reward: 750.0 | Eps: 0.574\n",
      "[DuelingDQN_Final] Ep  700 | Step  408976 | Reward: 870.0 | Eps: 0.568\n",
      "[DuelingDQN_Final] Ep  710 | Step  415930 | Reward: 930.0 | Eps: 0.561\n",
      "[DuelingDQN_Final] Ep  720 | Step  423076 | Reward: 1180.0 | Eps: 0.553\n",
      "[DuelingDQN_Final] Ep  730 | Step  429964 | Reward: 530.0 | Eps: 0.546\n",
      "[DuelingDQN_Final] Ep  740 | Step  438094 | Reward: 800.0 | Eps: 0.538\n",
      "[DuelingDQN_Final] Ep  750 | Step  445468 | Reward: 1140.0 | Eps: 0.530\n",
      "[DuelingDQN_Final] Ep  760 | Step  453611 | Reward: 400.0 | Eps: 0.521\n",
      "[DuelingDQN_Final] Ep  770 | Step  461797 | Reward: 3080.0 | Eps: 0.513\n",
      "[DuelingDQN_Final] Ep  780 | Step  467651 | Reward: 1130.0 | Eps: 0.506\n",
      "[DuelingDQN_Final] Ep  790 | Step  475574 | Reward: 880.0 | Eps: 0.498\n",
      "[DuelingDQN_Final] Ep  800 | Step  484432 | Reward: 420.0 | Eps: 0.489\n",
      "[DuelingDQN_Final] Ep  810 | Step  492866 | Reward: 1390.0 | Eps: 0.480\n",
      "[DuelingDQN_Final] Ep  820 | Step  500559 | Reward: 490.0 | Eps: 0.472\n",
      "[DuelingDQN_Final] Ep  830 | Step  508356 | Reward: 470.0 | Eps: 0.463\n",
      "[DuelingDQN_Final] Ep  840 | Step  517632 | Reward: 1180.0 | Eps: 0.454\n",
      "[DuelingDQN_Final] Ep  850 | Step  526262 | Reward: 1140.0 | Eps: 0.445\n",
      "[DuelingDQN_Final] Ep  860 | Step  533364 | Reward: 1200.0 | Eps: 0.437\n",
      "[DuelingDQN_Final] Ep  870 | Step  541112 | Reward: 600.0 | Eps: 0.429\n",
      "[DuelingDQN_Final] Ep  880 | Step  550020 | Reward: 1050.0 | Eps: 0.419\n",
      "[DuelingDQN_Final] Ep  890 | Step  556850 | Reward: 1070.0 | Eps: 0.412\n",
      "[DuelingDQN_Final] Ep  900 | Step  564777 | Reward: 720.0 | Eps: 0.404\n",
      "[DuelingDQN_Final] Ep  910 | Step  572179 | Reward: 1380.0 | Eps: 0.396\n",
      "[DuelingDQN_Final] Ep  920 | Step  579641 | Reward: 1040.0 | Eps: 0.388\n",
      "[DuelingDQN_Final] Ep  930 | Step  588159 | Reward: 300.0 | Eps: 0.379\n",
      "[DuelingDQN_Final] Ep  940 | Step  598417 | Reward: 2100.0 | Eps: 0.368\n",
      "[DuelingDQN_Final] Ep  950 | Step  604596 | Reward: 710.0 | Eps: 0.362\n",
      "[DuelingDQN_Final] Ep  960 | Step  611317 | Reward: 1220.0 | Eps: 0.355\n",
      "[DuelingDQN_Final] Ep  970 | Step  617485 | Reward: 610.0 | Eps: 0.348\n",
      "[DuelingDQN_Final] Ep  980 | Step  626830 | Reward: 1400.0 | Eps: 0.338\n",
      "[DuelingDQN_Final] Ep  990 | Step  635243 | Reward: 1470.0 | Eps: 0.329\n",
      "[DuelingDQN_Final] Ep 1000 | Step  644723 | Reward: 660.0 | Eps: 0.319\n",
      "[DuelingDQN_Final] Ep 1010 | Step  652551 | Reward: 1490.0 | Eps: 0.311\n",
      "[DuelingDQN_Final] Ep 1020 | Step  660664 | Reward: 1120.0 | Eps: 0.303\n",
      "[DuelingDQN_Final] Ep 1030 | Step  668587 | Reward: 1200.0 | Eps: 0.294\n",
      "[DuelingDQN_Final] Ep 1040 | Step  676352 | Reward: 2690.0 | Eps: 0.286\n",
      "[DuelingDQN_Final] Ep 1050 | Step  684303 | Reward: 1270.0 | Eps: 0.278\n",
      "[DuelingDQN_Final] Ep 1060 | Step  692821 | Reward: 1450.0 | Eps: 0.269\n",
      "[DuelingDQN_Final] Ep 1070 | Step  700411 | Reward: 470.0 | Eps: 0.261\n",
      "[DuelingDQN_Final] Ep 1080 | Step  708607 | Reward: 1080.0 | Eps: 0.252\n",
      "[DuelingDQN_Final] Ep 1090 | Step  715963 | Reward: 670.0 | Eps: 0.244\n",
      "[DuelingDQN_Final] Ep 1100 | Step  723664 | Reward: 980.0 | Eps: 0.236\n",
      "[DuelingDQN_Final] Ep 1110 | Step  730727 | Reward: 680.0 | Eps: 0.229\n",
      "[DuelingDQN_Final] Ep 1120 | Step  740205 | Reward: 2960.0 | Eps: 0.219\n",
      "[DuelingDQN_Final] Ep 1130 | Step  748366 | Reward: 1260.0 | Eps: 0.210\n",
      "[DuelingDQN_Final] Ep 1140 | Step  757806 | Reward: 600.0 | Eps: 0.200\n",
      "[DuelingDQN_Final] Ep 1150 | Step  765137 | Reward: 720.0 | Eps: 0.192\n",
      "[DuelingDQN_Final] Ep 1160 | Step  774366 | Reward: 1270.0 | Eps: 0.183\n",
      "[DuelingDQN_Final] Ep 1170 | Step  781741 | Reward: 1300.0 | Eps: 0.175\n",
      "[DuelingDQN_Final] Ep 1180 | Step  790007 | Reward: 1730.0 | Eps: 0.166\n",
      "[DuelingDQN_Final] Ep 1190 | Step  799611 | Reward: 1040.0 | Eps: 0.156\n",
      "[DuelingDQN_Final] Ep 1200 | Step  806850 | Reward: 1030.0 | Eps: 0.148\n",
      "[DuelingDQN_Final] Ep 1210 | Step  813904 | Reward: 1620.0 | Eps: 0.141\n",
      "[DuelingDQN_Final] Ep 1220 | Step  825417 | Reward: 1530.0 | Eps: 0.129\n",
      "[DuelingDQN_Final] Ep 1230 | Step  833380 | Reward: 1620.0 | Eps: 0.120\n",
      "[DuelingDQN_Final] Ep 1240 | Step  840892 | Reward: 1690.0 | Eps: 0.112\n",
      "[DuelingDQN_Final] Ep 1250 | Step  851781 | Reward: 930.0 | Eps: 0.101\n",
      "[DuelingDQN_Final] Ep 1260 | Step  862396 | Reward: 1570.0 | Eps: 0.090\n",
      "[DuelingDQN_Final] Ep 1270 | Step  871096 | Reward: 840.0 | Eps: 0.081\n",
      "[DuelingDQN_Final] Ep 1280 | Step  878981 | Reward: 1290.0 | Eps: 0.072\n",
      "[DuelingDQN_Final] Ep 1290 | Step  886926 | Reward: 910.0 | Eps: 0.064\n",
      "[DuelingDQN_Final] Ep 1300 | Step  896730 | Reward: 610.0 | Eps: 0.053\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m rewards_dueling, len_dueling, final_model_dueling = \u001b[43mtrain_dqn_variant\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDuelingDQN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDuelingDQN_Final\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_every\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 193\u001b[39m, in \u001b[36mtrain_dqn_variant\u001b[39m\u001b[34m(model_class, num_episodes, max_steps, label, seed_base, print_every, checkpoint_every)\u001b[39m\n\u001b[32m    190\u001b[39m steps_in_episode += \u001b[32m1\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m global_step > start_learning_after:\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m     _ = \u001b[43mdqn_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m global_step % target_update_freq == \u001b[32m0\u001b[39m:\n\u001b[32m    196\u001b[39m     target_net.load_state_dict(online_net.state_dict())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 131\u001b[39m, in \u001b[36mtrain_dqn_variant.<locals>.dqn_update\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    129\u001b[39m states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m q_values = \u001b[43monline_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m q_sa = q_values.gather(\u001b[32m1\u001b[39m, actions.unsqueeze(\u001b[32m1\u001b[39m)).squeeze(\u001b[32m1\u001b[39m)\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\miniconda3\\envs\\galaxian\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\miniconda3\\envs\\galaxian\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\Documents\\UVG\\10mo Semestre\\ReinforcementLearning\\Galaxian-Player\\src\\models.py:111\u001b[39m, in \u001b[36mDuelingDQN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    107\u001b[39m x = F.relu(\u001b[38;5;28mself\u001b[39m.conv3(x))\n\u001b[32m    109\u001b[39m x = x.view(x.size(\u001b[32m0\u001b[39m), -\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m val = F.relu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc_val\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    112\u001b[39m val = \u001b[38;5;28mself\u001b[39m.val_out(val)           \u001b[38;5;66;03m# (B, 1)\u001b[39;00m\n\u001b[32m    114\u001b[39m adv = F.relu(\u001b[38;5;28mself\u001b[39m.fc_adv(x))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\miniconda3\\envs\\galaxian\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\miniconda3\\envs\\galaxian\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\diego\\miniconda3\\envs\\galaxian\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "rewards_dueling, len_dueling, final_model_dueling = train_dqn_variant(\n",
    "    model_class=DuelingDQN,\n",
    "    num_episodes=num_episodes,\n",
    "    max_steps=max_steps,\n",
    "    label=\"DuelingDQN_Final\",\n",
    "    seed_base=seed,\n",
    "    print_every=10,\n",
    "    checkpoint_every=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd78d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(final_model_dueling.state_dict(), \"../models/dueling_dqn_final.pt\")\n",
    "print(\"Modelo DuelingDQN final guardado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35e8a2e",
   "metadata": {},
   "source": [
    "## Graficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0156ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(x, window):\n",
    "    x = np.array(x, dtype=np.float32)\n",
    "    if len(x) < window:\n",
    "        return x\n",
    "    return np.convolve(x, np.ones(window) / window, mode=\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb90516d",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 20\n",
    "\n",
    "ma_duel = moving_average(rewards_dueling, window)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# curvas “crudas” con poca alpha\n",
    "plt.plot(rewards_dueling, label=\"DuelingDQN (crudo)\", alpha=0.25)\n",
    "\n",
    "# medias móviles desplazadas para que caigan sobre el episodio correcto\n",
    "episodes_ma_duel = np.arange(window - 1, window - 1 + len(ma_duel))\n",
    "plt.plot(episodes_ma_duel, ma_duel, label=f\"DuelingDQN (media {window})\", linewidth=2)\n",
    "\n",
    "plt.xlabel(\"Episodio\")\n",
    "plt.ylabel(\"Reward total\")\n",
    "plt.title(\"Reward por episodio con media móvil\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77a1b58",
   "metadata": {},
   "source": [
    "## Desempeño"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9429446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = DQNPolicy(final_model_dueling, device=device, epsilon=0.0)\n",
    "scores = []\n",
    "for _ in range(50):\n",
    "    env_eval = make_galaxian_env(deepmind_wrappers=True, frame_stack=4)\n",
    "    obs, info = env_eval.reset()\n",
    "    done = False\n",
    "    total_r = 0\n",
    "    while not done:\n",
    "        action = policy(obs, info, env_eval.action_space)\n",
    "        obs, r, terminated, truncated, info = env_eval.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_r += r\n",
    "    scores.append(total_r)\n",
    "    env_eval.close()\n",
    "\n",
    "print(\"Mean eval score DQN:\", np.mean(scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "galaxian",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
